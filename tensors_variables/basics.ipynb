{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e40f3ca",
   "metadata": {},
   "source": [
    "# TensorFlow Basics: Tensors, Variables, and Mathematical Operations\n",
    "\n",
    "This notebook demonstrates fundamental TensorFlow concepts including:\n",
    "- Tensor creation and manipulation\n",
    "- Variables vs constants\n",
    "- Basic mathematical operations\n",
    "- Linear algebra operations\n",
    "- Statistical operations\n",
    "- Activation functions\n",
    "- Broadcasting and reshaping\n",
    "\n",
    "Let's start by exploring TensorFlow tensors and mathematical operations step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ffc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835dd1a",
   "metadata": {},
   "source": [
    "## 1. Setting Up TensorFlow Environment\n",
    "\n",
    "First, let's import TensorFlow and check its version. TensorFlow 2.x uses eager execution by default, which makes it easier to debug and work with tensors interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e27ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "GPU available: []\n",
      "Built with CUDA: False\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow version and GPU availability\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "\n",
    "# Set memory growth for GPU if available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ff18b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor_zero_d = tf.constant(4)\n",
    "print(tensor_zero_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8552c",
   "metadata": {},
   "source": [
    "## 2. Creating Tensors\n",
    "\n",
    "Tensors are the fundamental data structure in TensorFlow. They are multi-dimensional arrays with a uniform type (dtype). Let's explore different ways to create tensors:\n",
    "\n",
    "### 2.1 Scalar Tensors (0-D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb254b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar tensors:\n",
      "Integer: 42, shape: (), dtype: <dtype: 'int32'>\n",
      "Float: 3.141590118408203, shape: (), dtype: <dtype: 'float32'>\n",
      "String: b'Hello TensorFlow!', shape: (), dtype: <dtype: 'string'>\n",
      "Rank (ndim): 0\n"
     ]
    }
   ],
   "source": [
    "# Scalar tensors (rank-0 tensors)\n",
    "scalar_int = tf.constant(42)\n",
    "scalar_float = tf.constant(3.14159, dtype=tf.float32)\n",
    "scalar_string = tf.constant(\"Hello TensorFlow!\")\n",
    "\n",
    "print(\"Scalar tensors:\")\n",
    "print(f\"Integer: {scalar_int}, shape: {scalar_int.shape}, dtype: {scalar_int.dtype}\")\n",
    "print(f\"Float: {scalar_float}, shape: {scalar_float.shape}, dtype: {scalar_float.dtype}\")\n",
    "print(f\"String: {scalar_string}, shape: {scalar_string.shape}, dtype: {scalar_string.dtype}\")\n",
    "print(f\"Rank (ndim): {tf.rank(scalar_int)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16098592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor_one_d = tf.constant([1, 2, 3, 4])\n",
    "print(tensor_one_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ae3be",
   "metadata": {},
   "source": [
    "### 2.2 Vector Tensors (1-D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640b4d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector tensors:\n",
      "Integer vector: [1 2 3 4 5]\n",
      "Shape: (5,), Rank: 1, Size: 5\n",
      "Float vector: [1.  2.5 3.7 4.2]\n",
      "Shape: (4,), dtype: <dtype: 'float32'>\n",
      "First element: 1\n",
      "Last element: 5\n",
      "Slice [1:4]: [2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Vector tensors (rank-1 tensors)\n",
    "vector_int = tf.constant([1, 2, 3, 4, 5])\n",
    "vector_float = tf.constant([1.0, 2.5, 3.7, 4.2], dtype=tf.float32)\n",
    "\n",
    "print(\"Vector tensors:\")\n",
    "print(f\"Integer vector: {vector_int}\")\n",
    "print(f\"Shape: {vector_int.shape}, Rank: {tf.rank(vector_int)}, Size: {tf.size(vector_int)}\")\n",
    "print(f\"Float vector: {vector_float}\")\n",
    "print(f\"Shape: {vector_float.shape}, dtype: {vector_float.dtype}\")\n",
    "\n",
    "# Access elements\n",
    "print(f\"First element: {vector_int[0]}\")\n",
    "print(f\"Last element: {vector_int[-1]}\")\n",
    "print(f\"Slice [1:4]: {vector_int[1:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad14c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor_two_d = tf.constant([[1, 2], [3, 4]])\n",
    "print(tensor_two_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05375e8",
   "metadata": {},
   "source": [
    "### 2.3 Matrix Tensors (2-D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35cdc0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix tensors:\n",
      "3x3 matrix:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Shape: (3, 3), Rank: 2\n",
      "\n",
      "Identity matrix:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "Ones matrix:\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n",
      "Zeros matrix:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Element at [1,2]: 6\n",
      "Row 1: [4 5 6]\n",
      "Column 2: [3 6 9]\n",
      "2x2 submatrix:\n",
      "[[1 2]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix tensors (rank-2 tensors)\n",
    "matrix = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "identity_matrix = tf.eye(3)  # 3x3 identity matrix\n",
    "ones_matrix = tf.ones((2, 4))  # 2x4 matrix of ones\n",
    "zeros_matrix = tf.zeros((3, 3))  # 3x3 matrix of zeros\n",
    "\n",
    "print(\"Matrix tensors:\")\n",
    "print(f\"3x3 matrix:\\n{matrix}\")\n",
    "print(f\"Shape: {matrix.shape}, Rank: {tf.rank(matrix)}\")\n",
    "print(f\"\\nIdentity matrix:\\n{identity_matrix}\")\n",
    "print(f\"\\nOnes matrix:\\n{ones_matrix}\")\n",
    "print(f\"\\nZeros matrix:\\n{zeros_matrix}\")\n",
    "\n",
    "# Matrix indexing\n",
    "print(f\"\\nElement at [1,2]: {matrix[1, 2]}\")\n",
    "print(f\"Row 1: {matrix[1, :]}\")\n",
    "print(f\"Column 2: {matrix[:, 2]}\")\n",
    "print(f\"2x2 submatrix:\\n{matrix[:2, :2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94ece32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]\n",
      "\n",
      " [[13 14 15]\n",
      "  [16 17 18]]], shape=(3, 2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor_three_d = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]]])\n",
    "print(tensor_three_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f80d7c",
   "metadata": {},
   "source": [
    "### 2.4 Higher-Dimensional Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5639f9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Tensor:\n",
      "Shape: (2, 2, 2), Rank: 3\n",
      "Tensor:\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]]\n",
      "\n",
      "4D Random tensor shape: (2, 3, 4, 5)\n",
      "\n",
      "Range tensor: [0 1 2 3 4 5 6 7 8 9]\n",
      "Linspace tensor: [0.         0.11111111 0.22222222 0.33333334 0.44444445 0.5555556\n",
      " 0.6666667  0.7777778  0.8888889  1.        ]\n",
      "Random uniform tensor:\n",
      "[[1.1037648 4.7701526 9.451875 ]\n",
      " [5.5748177 1.8365633 5.6248055]\n",
      " [6.8161583 4.641049  5.171734 ]]\n"
     ]
    }
   ],
   "source": [
    "# 3D tensors (commonly used for RGB images: height x width x channels)\n",
    "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "print(\"3D Tensor:\")\n",
    "print(f\"Shape: {tensor_3d.shape}, Rank: {tf.rank(tensor_3d)}\")\n",
    "print(f\"Tensor:\\n{tensor_3d}\")\n",
    "\n",
    "# Random tensors for higher dimensions\n",
    "random_4d = tf.random.normal((2, 3, 4, 5))  # Batch x Height x Width x Channels\n",
    "print(f\"\\n4D Random tensor shape: {random_4d.shape}\")\n",
    "\n",
    "# Useful tensor creation functions\n",
    "range_tensor = tf.range(10)\n",
    "linspace_tensor = tf.linspace(0.0, 1.0, 10)\n",
    "random_uniform = tf.random.uniform((3, 3), minval=0, maxval=10)\n",
    "\n",
    "print(f\"\\nRange tensor: {range_tensor}\")\n",
    "print(f\"Linspace tensor: {linspace_tensor}\")\n",
    "print(f\"Random uniform tensor:\\n{random_uniform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "320ca3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 1  2  7]\n",
      "   [ 3  4  8]]\n",
      "\n",
      "  [[ 5  6  9]\n",
      "   [ 7  8 10]]]\n",
      "\n",
      "\n",
      " [[[ 9 10 11]\n",
      "   [11 12 12]]\n",
      "\n",
      "  [[13 14 13]\n",
      "   [15 16 14]]]], shape=(2, 2, 2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor_four_d = tf.constant([[[[1, 2, 7], [3, 4, 8]], [[5, 6, 9], [7, 8, 10]]], [[[9, 10, 11], [11, 12, 12]], [[13, 14, 13], [15, 16, 14]]]])\n",
    "print(tensor_four_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a41f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "numpy_array = np.array([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a68104",
   "metadata": {},
   "source": [
    "## 3. NumPy Interoperability\n",
    "\n",
    "TensorFlow tensors can be easily converted to/from NumPy arrays, making it seamless to work with existing NumPy-based code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc4f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "converted_tensor = tf.constant(numpy_array)\n",
    "print(converted_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d60ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy to TensorFlow conversion:\n",
      "NumPy array:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "TensorFlow tensor:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "Tensor dtype: <dtype: 'float32'>\n",
      "\n",
      "TensorFlow to NumPy conversion:\n",
      "TensorFlow tensor:\n",
      "[[10. 20. 30.]\n",
      " [40. 50. 60.]]\n",
      "TensorFlow tensor dtype: <dtype: 'float32'>\n",
      "TensorFlow tensor shape: (2, 3)\n",
      "NumPy array:\n",
      "[[10. 20. 30.]\n",
      " [40. 50. 60.]]\n",
      "NumPy array type: <class 'numpy.ndarray'>\n",
      "\n",
      "Tensor addition result:\n",
      "[[11. 22. 33.]\n",
      " [44. 55. 66.]]\n",
      "Result dtype: <dtype: 'float32'>\n",
      "Result shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy array to TensorFlow tensor\n",
    "numpy_array_2d = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "tensor_from_numpy = tf.constant(numpy_array_2d)\n",
    "\n",
    "print(\"NumPy to TensorFlow conversion:\")\n",
    "print(f\"NumPy array:\\n{numpy_array_2d}\")\n",
    "print(f\"TensorFlow tensor:\\n{tensor_from_numpy}\")\n",
    "print(f\"Tensor dtype: {tensor_from_numpy.dtype}\")\n",
    "\n",
    "# Convert TensorFlow tensor to NumPy array\n",
    "tensor_to_convert = tf.constant([[10, 20, 30], [40, 50, 60]], dtype=tf.float32)  # Same shape as numpy_array_2d\n",
    "numpy_from_tensor = tensor_to_convert.numpy()\n",
    "\n",
    "print(f\"\\nTensorFlow to NumPy conversion:\")\n",
    "print(f\"TensorFlow tensor:\\n{tensor_to_convert}\")\n",
    "print(f\"TensorFlow tensor dtype: {tensor_to_convert.dtype}\")\n",
    "print(f\"TensorFlow tensor shape: {tensor_to_convert.shape}\")\n",
    "print(f\"NumPy array:\\n{numpy_from_tensor}\")\n",
    "print(f\"NumPy array type: {type(numpy_from_tensor)}\")\n",
    "\n",
    "# Demonstrate that operations work seamlessly (both tensors now have same dtype and shape)\n",
    "result = tf.add(tensor_from_numpy, tensor_to_convert)\n",
    "print(f\"\\nTensor addition result:\\n{result}\")\n",
    "print(f\"Result dtype: {result.dtype}\")\n",
    "print(f\"Result shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625ddfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "850cd648",
   "metadata": {},
   "source": [
    "## 4. Variables vs Constants\n",
    "\n",
    "In TensorFlow, there are two main ways to store data:\n",
    "- **Constants**: Immutable tensors whose values cannot be changed\n",
    "- **Variables**: Mutable tensors that can be modified during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07aa8e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant tensor: [1 2 3 4]\n",
      "Variable tensor: <tf.Variable 'my_variable:0' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>\n",
      "Variable name: my_variable:0\n",
      "Variable trainable: True\n",
      "\n",
      "Modifying variable:\n",
      "After assign: <tf.Variable 'my_variable:0' shape=(4,) dtype=float32, numpy=array([10., 20., 30., 40.], dtype=float32)>\n",
      "After assign_add: <tf.Variable 'my_variable:0' shape=(4,) dtype=float32, numpy=array([11., 21., 31., 41.], dtype=float32)>\n",
      "After assign_sub: <tf.Variable 'my_variable:0' shape=(4,) dtype=float32, numpy=array([ 6., 16., 26., 36.], dtype=float32)>\n",
      "\n",
      "Zeros variable:\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)>\n",
      "Ones variable:\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[1., 1., 1.],\n",
      "       [1., 1., 1.]], dtype=float32)>\n",
      "Random variable:\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[ 0.7341732 ,  0.74133927, -0.70677614],\n",
      "       [-0.5299375 , -1.4579775 , -0.8598304 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# Constants - immutable\n",
    "constant_tensor = tf.constant([1, 2, 3, 4])\n",
    "print(f\"Constant tensor: {constant_tensor}\")\n",
    "\n",
    "# Variables - mutable (used for model parameters like weights and biases)\n",
    "variable_tensor = tf.Variable([1.0, 2.0, 3.0, 4.0], name=\"my_variable\")\n",
    "print(f\"Variable tensor: {variable_tensor}\")\n",
    "print(f\"Variable name: {variable_tensor.name}\")\n",
    "print(f\"Variable trainable: {variable_tensor.trainable}\")\n",
    "\n",
    "# Modify variable values\n",
    "print(\"\\nModifying variable:\")\n",
    "variable_tensor.assign([10.0, 20.0, 30.0, 40.0])\n",
    "print(f\"After assign: {variable_tensor}\")\n",
    "\n",
    "# Add to variable\n",
    "variable_tensor.assign_add([1.0, 1.0, 1.0, 1.0])\n",
    "print(f\"After assign_add: {variable_tensor}\")\n",
    "\n",
    "# Subtract from variable\n",
    "variable_tensor.assign_sub([5.0, 5.0, 5.0, 5.0])\n",
    "print(f\"After assign_sub: {variable_tensor}\")\n",
    "\n",
    "# Initialize variables with different strategies\n",
    "zeros_var = tf.Variable(tf.zeros((2, 3)))\n",
    "ones_var = tf.Variable(tf.ones((2, 3)))\n",
    "random_var = tf.Variable(tf.random.normal((2, 3)))\n",
    "\n",
    "print(f\"\\nZeros variable:\\n{zeros_var}\")\n",
    "print(f\"Ones variable:\\n{ones_var}\")\n",
    "print(f\"Random variable:\\n{random_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a8861",
   "metadata": {},
   "source": [
    "## 5. Basic Mathematical Operations\n",
    "\n",
    "TensorFlow provides comprehensive mathematical operations for tensors. These operations are element-wise by default and support broadcasting.\n",
    "\n",
    "### 5.1 Arithmetic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "633e15bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic arithmetic operations:\n",
      "a = [1. 2. 3. 4.]\n",
      "b = [5. 6. 7. 8.]\n",
      "scalar = 2.0\n",
      "\n",
      "Addition (a + b): [ 6.  8. 10. 12.]\n",
      "Scalar addition (a + scalar): [3. 4. 5. 6.]\n",
      "Subtraction (b - a): [4. 4. 4. 4.]\n",
      "Multiplication (a * b): [ 5. 12. 21. 32.]\n",
      "Division (b / a): [5.        3.        2.3333333 2.       ]\n",
      "Power (a^2): [ 1.  4.  9. 16.]\n",
      "Square root of a: [1.        1.4142135 1.7320508 2.       ]\n",
      "Absolute value of [-1. -2.  3. -4.]: [1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "# Create sample tensors for operations\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "b = tf.constant([5.0, 6.0, 7.0, 8.0])\n",
    "scalar = tf.constant(2.0)\n",
    "\n",
    "print(\"Basic arithmetic operations:\")\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"scalar = {scalar}\")\n",
    "\n",
    "# Addition\n",
    "addition = tf.add(a, b)  # or a + b\n",
    "print(f\"\\nAddition (a + b): {addition}\")\n",
    "print(f\"Scalar addition (a + scalar): {a + scalar}\")\n",
    "\n",
    "# Subtraction\n",
    "subtraction = tf.subtract(b, a)  # or b - a\n",
    "print(f\"Subtraction (b - a): {subtraction}\")\n",
    "\n",
    "# Multiplication\n",
    "multiplication = tf.multiply(a, b)  # or a * b (element-wise)\n",
    "print(f\"Multiplication (a * b): {multiplication}\")\n",
    "\n",
    "# Division\n",
    "division = tf.divide(b, a)  # or b / a\n",
    "print(f\"Division (b / a): {division}\")\n",
    "\n",
    "# Power\n",
    "power = tf.pow(a, 2)  # or a ** 2\n",
    "print(f\"Power (a^2): {power}\")\n",
    "\n",
    "# Square root\n",
    "sqrt_a = tf.sqrt(a)\n",
    "print(f\"Square root of a: {sqrt_a}\")\n",
    "\n",
    "# Absolute value\n",
    "negative_tensor = tf.constant([-1.0, -2.0, 3.0, -4.0])\n",
    "abs_value = tf.abs(negative_tensor)\n",
    "print(f\"Absolute value of {negative_tensor}: {abs_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450a75f",
   "metadata": {},
   "source": [
    "### 5.2 Reduction Operations\n",
    "\n",
    "Reduction operations compute a single value from a tensor by applying an operation across specified dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "618071e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Matrix:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "\n",
      "Reduction operations:\n",
      "Total sum: 45.0\n",
      "Sum across rows (axis=0): [12. 15. 18.]\n",
      "Sum across columns (axis=1): [ 6. 15. 24.]\n",
      "\n",
      "Mean operations:\n",
      "Total mean: 5.0\n",
      "Mean across rows (axis=0): [4. 5. 6.]\n",
      "Mean across columns (axis=1): [2. 5. 8.]\n",
      "\n",
      "Min/Max operations:\n",
      "Maximum: 9.0\n",
      "Minimum: 1.0\n",
      "Max across columns (axis=1): [3. 6. 9.]\n",
      "\n",
      "Statistical operations:\n",
      "Standard deviation: 2.58198881149292\n",
      "Variance: 6.666666507720947\n"
     ]
    }
   ],
   "source": [
    "# Create a 2D tensor for reduction operations\n",
    "matrix_2d = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\n",
    "print(f\"2D Matrix:\\n{matrix_2d}\")\n",
    "\n",
    "# Sum operations\n",
    "total_sum = tf.reduce_sum(matrix_2d)\n",
    "sum_axis_0 = tf.reduce_sum(matrix_2d, axis=0)  # Sum across rows\n",
    "sum_axis_1 = tf.reduce_sum(matrix_2d, axis=1)  # Sum across columns\n",
    "\n",
    "print(f\"\\nReduction operations:\")\n",
    "print(f\"Total sum: {total_sum}\")\n",
    "print(f\"Sum across rows (axis=0): {sum_axis_0}\")\n",
    "print(f\"Sum across columns (axis=1): {sum_axis_1}\")\n",
    "\n",
    "# Mean operations\n",
    "total_mean = tf.reduce_mean(matrix_2d)\n",
    "mean_axis_0 = tf.reduce_mean(matrix_2d, axis=0)\n",
    "mean_axis_1 = tf.reduce_mean(matrix_2d, axis=1)\n",
    "\n",
    "print(f\"\\nMean operations:\")\n",
    "print(f\"Total mean: {total_mean}\")\n",
    "print(f\"Mean across rows (axis=0): {mean_axis_0}\")\n",
    "print(f\"Mean across columns (axis=1): {mean_axis_1}\")\n",
    "\n",
    "# Min and Max operations\n",
    "maximum = tf.reduce_max(matrix_2d)\n",
    "minimum = tf.reduce_min(matrix_2d)\n",
    "max_axis_1 = tf.reduce_max(matrix_2d, axis=1)\n",
    "\n",
    "print(f\"\\nMin/Max operations:\")\n",
    "print(f\"Maximum: {maximum}\")\n",
    "print(f\"Minimum: {minimum}\")\n",
    "print(f\"Max across columns (axis=1): {max_axis_1}\")\n",
    "\n",
    "# Standard deviation and variance\n",
    "std_dev = tf.math.reduce_std(matrix_2d)\n",
    "variance = tf.math.reduce_variance(matrix_2d)\n",
    "\n",
    "print(f\"\\nStatistical operations:\")\n",
    "print(f\"Standard deviation: {std_dev}\")\n",
    "print(f\"Variance: {variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f6ed6",
   "metadata": {},
   "source": [
    "### 5.3 Trigonometric and Logarithmic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28432d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angles: [0.        0.7853982 1.5707964 3.1415927]\n",
      "\n",
      "Trigonometric functions:\n",
      "sin(angles): [ 0.0000000e+00  7.0710677e-01  1.0000000e+00 -8.7422777e-08]\n",
      "cos(angles): [ 1.0000000e+00  7.0710677e-01 -4.3711388e-08 -1.0000000e+00]\n",
      "tan(angles): [ 0.0000000e+00  1.0000000e+00 -2.2877332e+07  8.7422777e-08]\n",
      "\n",
      "Positive values: [  1.   2.  10. 100.]\n",
      "Natural log: [0.        0.6931472 2.3025851 4.6051702]\n",
      "Log base 10: [0.         0.30102998 1.         2.        ]\n",
      "Exponential: [2.7182817e+00 7.3890562e+00 2.2026465e+04           inf]\n",
      "\n",
      "Hyperbolic functions for [-1.  0.  1.  2.]:\n",
      "sinh: [-1.1752012  0.         1.1752012  3.6268604]\n",
      "cosh: [1.5430807 1.        1.5430807 3.7621956]\n",
      "tanh: [-0.7615942  0.         0.7615942  0.9640276]\n"
     ]
    }
   ],
   "source": [
    "# Trigonometric functions\n",
    "angles = tf.constant([0.0, np.pi/4, np.pi/2, np.pi])  # Use numpy values directly\n",
    "print(f\"Angles: {angles}\")\n",
    "\n",
    "sin_values = tf.sin(angles)\n",
    "cos_values = tf.cos(angles)\n",
    "tan_values = tf.tan(angles)\n",
    "\n",
    "print(f\"\\nTrigonometric functions:\")\n",
    "print(f\"sin(angles): {sin_values}\")\n",
    "print(f\"cos(angles): {cos_values}\")\n",
    "print(f\"tan(angles): {tan_values}\")\n",
    "\n",
    "# Logarithmic and exponential functions\n",
    "positive_values = tf.constant([1.0, 2.0, 10.0, 100.0])\n",
    "print(f\"\\nPositive values: {positive_values}\")\n",
    "\n",
    "log_natural = tf.math.log(positive_values)  # Natural logarithm\n",
    "log_10 = tf.math.log(positive_values) / tf.math.log(10.0)  # Log base 10\n",
    "exp_values = tf.exp(positive_values)  # Exponential\n",
    "\n",
    "print(f\"Natural log: {log_natural}\")\n",
    "print(f\"Log base 10: {log_10}\")\n",
    "print(f\"Exponential: {exp_values}\")\n",
    "\n",
    "# Hyperbolic functions\n",
    "values = tf.constant([-1.0, 0.0, 1.0, 2.0])\n",
    "sinh_values = tf.sinh(values)\n",
    "cosh_values = tf.cosh(values)\n",
    "tanh_values = tf.tanh(values)\n",
    "\n",
    "print(f\"\\nHyperbolic functions for {values}:\")\n",
    "print(f\"sinh: {sinh_values}\")\n",
    "print(f\"cosh: {cosh_values}\")\n",
    "print(f\"tanh: {tanh_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8fd64",
   "metadata": {},
   "source": [
    "## 6. Linear Algebra Operations\n",
    "\n",
    "Linear algebra is fundamental to deep learning. TensorFlow provides comprehensive support for matrix operations.\n",
    "\n",
    "### 6.1 Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "919848d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication:\n",
      "Matrix A:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "Matrix B:\n",
      "[[5. 6.]\n",
      " [7. 8.]]\n",
      "Vector:\n",
      "[[1.]\n",
      " [2.]]\n",
      "\n",
      "A @ B (matrix multiplication):\n",
      "[[19. 22.]\n",
      " [43. 50.]]\n",
      "\n",
      "A @ vector:\n",
      "[[ 5.]\n",
      " [11.]]\n",
      "\n",
      "Transpose of A:\n",
      "[[1. 3.]\n",
      " [2. 4.]]\n",
      "\n",
      "Batch matrix multiplication result:\n",
      "[[[ 1.  2.]\n",
      "  [ 3.  4.]]\n",
      "\n",
      " [[16. 17.]\n",
      "  [22. 23.]]]\n",
      "\n",
      "Hadamard product (A * B):\n",
      "[[ 5. 12.]\n",
      " [21. 32.]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication examples\n",
    "A = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "B = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "vector = tf.constant([[1.0], [2.0]])\n",
    "\n",
    "print(\"Matrix multiplication:\")\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"Matrix B:\\n{B}\")\n",
    "print(f\"Vector:\\n{vector}\")\n",
    "\n",
    "# Matrix multiplication using tf.matmul\n",
    "AB = tf.matmul(A, B)\n",
    "print(f\"\\nA @ B (matrix multiplication):\\n{AB}\")\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "Av = tf.matmul(A, vector)\n",
    "print(f\"\\nA @ vector:\\n{Av}\")\n",
    "\n",
    "# Transpose\n",
    "A_transpose = tf.transpose(A)\n",
    "print(f\"\\nTranspose of A:\\n{A_transpose}\")\n",
    "\n",
    "# Batch matrix multiplication\n",
    "batch_A = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n",
    "batch_B = tf.constant([[[1.0, 0.0], [0.0, 1.0]], [[2.0, 1.0], [1.0, 2.0]]])\n",
    "batch_result = tf.matmul(batch_A, batch_B)\n",
    "print(f\"\\nBatch matrix multiplication result:\\n{batch_result}\")\n",
    "\n",
    "# Element-wise multiplication (Hadamard product)\n",
    "hadamard = tf.multiply(A, B)  # or A * B\n",
    "print(f\"\\nHadamard product (A * B):\\n{hadamard}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590019e1",
   "metadata": {},
   "source": [
    "### 6.2 Advanced Linear Algebra Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "299d919c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square matrix:\n",
      "[[4. 2.]\n",
      " [1. 3.]]\n",
      "\n",
      "Determinant: 10.0\n",
      "Matrix inverse:\n",
      "[[ 0.3 -0.2]\n",
      " [-0.1  0.4]]\n",
      "A @ A^(-1) (should be identity):\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "Symmetric matrix:\n",
      "[[3. 1.]\n",
      " [1. 3.]]\n",
      "Eigenvalues: [1.9999998 3.9999998]\n",
      "Eigenvectors:\n",
      "[[-0.7071067   0.70710677]\n",
      " [ 0.70710677  0.7071067 ]]\n",
      "\n",
      "Matrix for SVD:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "Singular values: [9.508034   0.77286935]\n",
      "U matrix shape: (2, 2)\n",
      "V matrix shape: (3, 2)\n",
      "\n",
      "QR decomposition:\n",
      "Q matrix:\n",
      "[[-0.16903079  0.897085  ]\n",
      " [-0.5070925   0.2760267 ]\n",
      " [-0.8451542  -0.34503305]]\n",
      "R matrix:\n",
      "[[-5.91608    -7.437357  ]\n",
      " [ 0.          0.82807845]]\n"
     ]
    }
   ],
   "source": [
    "# Create matrices for advanced operations\n",
    "square_matrix = tf.constant([[4.0, 2.0], [1.0, 3.0]], dtype=tf.float32)\n",
    "print(f\"Square matrix:\\n{square_matrix}\")\n",
    "\n",
    "# Determinant\n",
    "det = tf.linalg.det(square_matrix)\n",
    "print(f\"\\nDeterminant: {det}\")\n",
    "\n",
    "# Matrix inverse\n",
    "try:\n",
    "    inverse = tf.linalg.inv(square_matrix)\n",
    "    print(f\"Matrix inverse:\\n{inverse}\")\n",
    "    \n",
    "    # Verify inverse (should be identity matrix)\n",
    "    verification = tf.matmul(square_matrix, inverse)\n",
    "    print(f\"A @ A^(-1) (should be identity):\\n{verification}\")\n",
    "except:\n",
    "    print(\"Matrix is not invertible\")\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "symmetric_matrix = tf.constant([[3.0, 1.0], [1.0, 3.0]], dtype=tf.float32)\n",
    "eigenvalues, eigenvectors = tf.linalg.eigh(symmetric_matrix)\n",
    "print(f\"\\nSymmetric matrix:\\n{symmetric_matrix}\")\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors:\\n{eigenvectors}\")\n",
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "matrix_for_svd = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)\n",
    "s, u, v = tf.linalg.svd(matrix_for_svd)\n",
    "print(f\"\\nMatrix for SVD:\\n{matrix_for_svd}\")\n",
    "print(f\"Singular values: {s}\")\n",
    "print(f\"U matrix shape: {u.shape}\")\n",
    "print(f\"V matrix shape: {v.shape}\")\n",
    "\n",
    "# QR decomposition\n",
    "q, r = tf.linalg.qr(tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=tf.float32))\n",
    "print(f\"\\nQR decomposition:\")\n",
    "print(f\"Q matrix:\\n{q}\")\n",
    "print(f\"R matrix:\\n{r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138c3a5",
   "metadata": {},
   "source": [
    "## 7. Tensor Manipulation and Reshaping\n",
    "\n",
    "Understanding how to manipulate tensor shapes is crucial for deep learning operations.\n",
    "\n",
    "### 7.1 Reshaping Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91073103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]\n",
      "Shape: (12,)\n",
      "\n",
      "Reshape to (3, 4):\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]]\n",
      "Shape: (3, 4)\n",
      "\n",
      "Reshape to (2, 2, 3):\n",
      "[[[ 0.  1.  2.]\n",
      "  [ 3.  4.  5.]]\n",
      "\n",
      " [[ 6.  7.  8.]\n",
      "  [ 9. 10. 11.]]]\n",
      "Shape: (2, 2, 3)\n",
      "\n",
      "Reshape to (1, 2, 2, 3):\n",
      "[[[[ 0.  1.  2.]\n",
      "   [ 3.  4.  5.]]\n",
      "\n",
      "  [[ 6.  7.  8.]\n",
      "   [ 9. 10. 11.]]]]\n",
      "Shape: (1, 2, 2, 3)\n",
      "\n",
      "Flattened tensor: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]\n",
      "Shape: (12,)\n",
      "\n",
      "Expand dims at axis 0: [[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]]\n",
      "Shape: (1, 12)\n",
      "\n",
      "Expand dims at axis -1: [[ 0.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [11.]]\n",
      "Shape: (12, 1)\n",
      "\n",
      "Original with size-1 dims: [[[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]]], shape: (1, 4, 1)\n",
      "After squeeze: [1 2 3 4], shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "# Original tensor\n",
    "original = tf.range(12, dtype=tf.float32)\n",
    "print(f\"Original tensor: {original}\")\n",
    "print(f\"Shape: {original.shape}\")\n",
    "\n",
    "# Reshape to different dimensions\n",
    "reshaped_2d = tf.reshape(original, (3, 4))\n",
    "reshaped_3d = tf.reshape(original, (2, 2, 3))\n",
    "reshaped_4d = tf.reshape(original, (1, 2, 2, 3))\n",
    "\n",
    "print(f\"\\nReshape to (3, 4):\\n{reshaped_2d}\")\n",
    "print(f\"Shape: {reshaped_2d.shape}\")\n",
    "\n",
    "print(f\"\\nReshape to (2, 2, 3):\\n{reshaped_3d}\")\n",
    "print(f\"Shape: {reshaped_3d.shape}\")\n",
    "\n",
    "print(f\"\\nReshape to (1, 2, 2, 3):\\n{reshaped_4d}\")\n",
    "print(f\"Shape: {reshaped_4d.shape}\")\n",
    "\n",
    "# Flatten tensor\n",
    "flattened = tf.reshape(reshaped_3d, (-1,))  # -1 means \"infer this dimension\"\n",
    "print(f\"\\nFlattened tensor: {flattened}\")\n",
    "print(f\"Shape: {flattened.shape}\")\n",
    "\n",
    "# Expand dimensions\n",
    "expanded = tf.expand_dims(original, axis=0)  # Add dimension at axis 0\n",
    "expanded_end = tf.expand_dims(original, axis=-1)  # Add dimension at end\n",
    "\n",
    "print(f\"\\nExpand dims at axis 0: {expanded}\")\n",
    "print(f\"Shape: {expanded.shape}\")\n",
    "print(f\"\\nExpand dims at axis -1: {expanded_end}\")\n",
    "print(f\"Shape: {expanded_end.shape}\")\n",
    "\n",
    "# Squeeze dimensions (remove size-1 dimensions)\n",
    "to_squeeze = tf.constant([[[1], [2], [3], [4]]])\n",
    "squeezed = tf.squeeze(to_squeeze)\n",
    "print(f\"\\nOriginal with size-1 dims: {to_squeeze}, shape: {to_squeeze.shape}\")\n",
    "print(f\"After squeeze: {squeezed}, shape: {squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac2a608",
   "metadata": {},
   "source": [
    "### 7.2 Concatenation and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccb87130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "Tensor 2:\n",
      "[[5 6]\n",
      " [7 8]]\n",
      "Tensor 3:\n",
      "[[ 9 10]\n",
      " [11 12]]\n",
      "\n",
      "Concatenate along axis 0 (vertical):\n",
      "[[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "Shape: (6, 2)\n",
      "\n",
      "Concatenate along axis 1 (horizontal):\n",
      "[[ 1  2  5  6  9 10]\n",
      " [ 3  4  7  8 11 12]]\n",
      "Shape: (2, 6)\n",
      "\n",
      "Stacked tensors (new dimension):\n",
      "[[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]]\n",
      "Shape: (3, 2, 2)\n",
      "\n",
      "Large tensor to split:\n",
      "[[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n",
      "Split into 3 parts along axis 1:\n",
      "Part 0: [[1 2]\n",
      " [7 8]]\n",
      "Part 1: [[ 3  4]\n",
      " [ 9 10]]\n",
      "Part 2: [[ 5  6]\n",
      " [11 12]]\n",
      "\n",
      "Unstacked tensors:\n",
      "Tensor 0:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "Tensor 1:\n",
      "[[5 6]\n",
      " [7 8]]\n",
      "Tensor 2:\n",
      "[[ 9 10]\n",
      " [11 12]]\n"
     ]
    }
   ],
   "source": [
    "# Create tensors for concatenation\n",
    "tensor1 = tf.constant([[1, 2], [3, 4]])\n",
    "tensor2 = tf.constant([[5, 6], [7, 8]])\n",
    "tensor3 = tf.constant([[9, 10], [11, 12]])\n",
    "\n",
    "print(f\"Tensor 1:\\n{tensor1}\")\n",
    "print(f\"Tensor 2:\\n{tensor2}\")\n",
    "print(f\"Tensor 3:\\n{tensor3}\")\n",
    "\n",
    "# Concatenate along different axes\n",
    "concat_axis_0 = tf.concat([tensor1, tensor2, tensor3], axis=0)  # Stack vertically\n",
    "concat_axis_1 = tf.concat([tensor1, tensor2, tensor3], axis=1)  # Stack horizontally\n",
    "\n",
    "print(f\"\\nConcatenate along axis 0 (vertical):\\n{concat_axis_0}\")\n",
    "print(f\"Shape: {concat_axis_0.shape}\")\n",
    "\n",
    "print(f\"\\nConcatenate along axis 1 (horizontal):\\n{concat_axis_1}\")\n",
    "print(f\"Shape: {concat_axis_1.shape}\")\n",
    "\n",
    "# Stack tensors (creates new dimension)\n",
    "stacked = tf.stack([tensor1, tensor2, tensor3], axis=0)\n",
    "print(f\"\\nStacked tensors (new dimension):\\n{stacked}\")\n",
    "print(f\"Shape: {stacked.shape}\")\n",
    "\n",
    "# Split tensors\n",
    "large_tensor = tf.constant([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]])\n",
    "print(f\"\\nLarge tensor to split:\\n{large_tensor}\")\n",
    "\n",
    "# Split into equal parts\n",
    "split_result = tf.split(large_tensor, num_or_size_splits=3, axis=1)\n",
    "print(f\"Split into 3 parts along axis 1:\")\n",
    "for i, part in enumerate(split_result):\n",
    "    print(f\"Part {i}: {part}\")\n",
    "\n",
    "# Unstack (inverse of stack)\n",
    "unstacked = tf.unstack(stacked, axis=0)\n",
    "print(f\"\\nUnstacked tensors:\")\n",
    "for i, tensor in enumerate(unstacked):\n",
    "    print(f\"Tensor {i}:\\n{tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e6664",
   "metadata": {},
   "source": [
    "## 8. Broadcasting\n",
    "\n",
    "Broadcasting allows operations between tensors of different shapes by automatically expanding the smaller tensor to match the larger one's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c77c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (2, 3)\n",
      "Matrix:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n",
      "Scalar: 10.0\n",
      "Vector (row): [1. 2. 3.]\n",
      "Vector (column):\n",
      "[[1.]\n",
      " [2.]]\n",
      "\n",
      "Matrix + Scalar (broadcasting):\n",
      "[[11. 12. 13.]\n",
      " [14. 15. 16.]]\n",
      "\n",
      "Matrix + Row Vector (broadcasting):\n",
      "[[2. 4. 6.]\n",
      " [5. 7. 9.]]\n",
      "\n",
      "Matrix + Column Vector (broadcasting):\n",
      "[[2. 3. 4.]\n",
      " [6. 7. 8.]]\n",
      "\n",
      "3D tensor shape: (2, 3, 4)\n",
      "Vector shape: (4,)\n",
      "Broadcast result shape: (2, 3, 4)\n",
      "\n",
      "Manual broadcast of [1. 2. 3.] to shape (2, 3):\n",
      "[[1. 2. 3.]\n",
      " [1. 2. 3.]]\n",
      "\n",
      "Broadcast compatible shapes result: [2 3]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting examples\n",
    "matrix = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "scalar = tf.constant(10.0)\n",
    "vector_row = tf.constant([1.0, 2.0, 3.0])  # Shape: (3,)\n",
    "vector_col = tf.constant([[1.0], [2.0]])    # Shape: (2, 1)\n",
    "\n",
    "print(f\"Matrix shape: {matrix.shape}\")\n",
    "print(f\"Matrix:\\n{matrix}\")\n",
    "print(f\"\\nScalar: {scalar}\")\n",
    "print(f\"Vector (row): {vector_row}\")\n",
    "print(f\"Vector (column):\\n{vector_col}\")\n",
    "\n",
    "# Scalar broadcasting\n",
    "scalar_broadcast = matrix + scalar\n",
    "print(f\"\\nMatrix + Scalar (broadcasting):\\n{scalar_broadcast}\")\n",
    "\n",
    "# Vector broadcasting (row)\n",
    "vector_broadcast_row = matrix + vector_row\n",
    "print(f\"\\nMatrix + Row Vector (broadcasting):\\n{vector_broadcast_row}\")\n",
    "\n",
    "# Vector broadcasting (column)\n",
    "vector_broadcast_col = matrix + vector_col\n",
    "print(f\"\\nMatrix + Column Vector (broadcasting):\\n{vector_broadcast_col}\")\n",
    "\n",
    "# More complex broadcasting\n",
    "tensor_3d = tf.random.normal((2, 3, 4))\n",
    "vector_for_3d = tf.constant([1.0, 2.0, 3.0, 4.0])  # Shape: (4,)\n",
    "\n",
    "print(f\"\\n3D tensor shape: {tensor_3d.shape}\")\n",
    "print(f\"Vector shape: {vector_for_3d.shape}\")\n",
    "\n",
    "broadcast_result = tensor_3d + vector_for_3d\n",
    "print(f\"Broadcast result shape: {broadcast_result.shape}\")\n",
    "\n",
    "# Manual broadcasting using tf.broadcast_to\n",
    "manual_broadcast = tf.broadcast_to(vector_row, (2, 3))\n",
    "print(f\"\\nManual broadcast of {vector_row} to shape (2, 3):\\n{manual_broadcast}\")\n",
    "\n",
    "# Check if two tensors are broadcast compatible\n",
    "try:\n",
    "    broadcast_shapes = tf.broadcast_dynamic_shape(tf.shape(matrix), tf.shape(vector_row))\n",
    "    print(f\"\\nBroadcast compatible shapes result: {broadcast_shapes}\")\n",
    "except:\n",
    "    print(\"Shapes are not broadcast compatible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fbd3f",
   "metadata": {},
   "source": [
    "## 9. Activation Functions\n",
    "\n",
    "Activation functions are crucial components in neural networks. TensorFlow provides many common activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8925968a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input values: [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\n",
      "\n",
      "Activation function outputs:\n",
      "Sigmoid: [0.00669285 0.01798621 0.04742587 0.11920294 0.2689414  0.5\n",
      " 0.73105854 0.8807971  0.95257413 0.98201376 0.9933072 ]\n",
      "Tanh: [-0.99990916 -0.9993292  -0.9950547  -0.9640276  -0.7615942   0.\n",
      "  0.7615942   0.9640276   0.9950547   0.9993292   0.99990916]\n",
      "ReLU: [0. 0. 0. 0. 0. 0. 1. 2. 3. 4. 5.]\n",
      "Leaky ReLU: [-1.  -0.8 -0.6 -0.4 -0.2  0.   1.   2.   3.   4.   5. ]\n",
      "ELU: [-0.99326205 -0.9816844  -0.95021296 -0.86466473 -0.63212055  0.\n",
      "  1.          2.          3.          4.          5.        ]\n",
      "Swish: [-0.03346426 -0.07194484 -0.1422776  -0.23840588 -0.2689414   0.\n",
      "  0.73105854  1.7615942   2.8577223   3.928055    4.966536  ]\n",
      "\n",
      "Logits: [[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "Softmax output: [[0.09003057 0.24472848 0.6652409 ]\n",
      " [0.09003057 0.24472848 0.6652409 ]]\n",
      "Softmax sum per row: [0.99999994 0.99999994]\n",
      "Log softmax output: [[-2.407606   -1.4076059  -0.40760595]\n",
      " [-2.407606   -1.4076059  -0.40760595]]\n",
      "GELU output: [-1.6391277e-06 -1.2648106e-04 -4.0496886e-03 -4.5500278e-02\n",
      " -1.5865526e-01  0.0000000e+00  8.4134471e-01  1.9544997e+00\n",
      "  2.9959502e+00  3.9998736e+00  4.9999981e+00]\n",
      "Custom activation (x * sigmoid(x)): [-0.03346426 -0.07194484 -0.1422776  -0.23840588 -0.2689414   0.\n",
      "  0.73105854  1.7615942   2.8577223   3.928055    4.966536  ]\n"
     ]
    }
   ],
   "source": [
    "# Create input values for activation functions\n",
    "x = tf.linspace(-5.0, 5.0, 11)\n",
    "print(f\"Input values: {x}\")\n",
    "\n",
    "# Common activation functions\n",
    "sigmoid_output = tf.nn.sigmoid(x)\n",
    "tanh_output = tf.nn.tanh(x)\n",
    "relu_output = tf.nn.relu(x)\n",
    "leaky_relu_output = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "elu_output = tf.nn.elu(x)\n",
    "swish_output = tf.nn.swish(x)\n",
    "\n",
    "print(f\"\\nActivation function outputs:\")\n",
    "print(f\"Sigmoid: {sigmoid_output}\")\n",
    "print(f\"Tanh: {tanh_output}\")\n",
    "print(f\"ReLU: {relu_output}\")\n",
    "print(f\"Leaky ReLU: {leaky_relu_output}\")\n",
    "print(f\"ELU: {elu_output}\")\n",
    "print(f\"Swish: {swish_output}\")\n",
    "\n",
    "# Softmax activation (commonly used for classification)\n",
    "logits = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "softmax_output = tf.nn.softmax(logits)\n",
    "print(f\"\\nLogits: {logits}\")\n",
    "print(f\"Softmax output: {softmax_output}\")\n",
    "print(f\"Softmax sum per row: {tf.reduce_sum(softmax_output, axis=1)}\")\n",
    "\n",
    "# Log softmax (numerically stable)\n",
    "log_softmax_output = tf.nn.log_softmax(logits)\n",
    "print(f\"Log softmax output: {log_softmax_output}\")\n",
    "\n",
    "# Gelu activation (used in transformers)\n",
    "gelu_output = tf.nn.gelu(x)\n",
    "print(f\"GELU output: {gelu_output}\")\n",
    "\n",
    "# Custom activation function using mathematical operations\n",
    "def custom_activation(x):\n",
    "    \"\"\"Custom activation: x * sigmoid(x)\"\"\"\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "custom_output = custom_activation(x)\n",
    "print(f\"Custom activation (x * sigmoid(x)): {custom_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83666a15",
   "metadata": {},
   "source": [
    "## 10. Loss Functions\n",
    "\n",
    "Loss functions measure how well a model's predictions match the true values. They are essential for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f004275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Loss Functions:\n",
      "True values: [1. 2. 3. 4.]\n",
      "Predicted values: [1.1 2.2 2.9 3.8]\n",
      "MSE: 0.025000005960464478\n",
      "MAE: 0.15000000596046448\n",
      "Huber Loss: 0.012500002980232239\n",
      "\n",
      "Classification Loss Functions:\n",
      "True classes: [0 1 2 1]\n",
      "Predicted logits:\n",
      "[[2.  0.5 0.1]\n",
      " [0.1 2.  0.3]\n",
      " [0.2 0.1 1.8]\n",
      " [0.3 1.5 0.2]]\n",
      "Sparse Categorical Crossentropy: 0.3456231355667114\n",
      "Categorical Crossentropy: 0.3456231355667114\n",
      "\n",
      "Binary Crossentropy: 0.16425204277038574\n",
      "\n",
      "Custom weighted MSE: 0.0312500037252903\n"
     ]
    }
   ],
   "source": [
    "# Regression loss functions\n",
    "y_true_regression = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "y_pred_regression = tf.constant([1.1, 2.2, 2.9, 3.8])\n",
    "\n",
    "print(\"Regression Loss Functions:\")\n",
    "print(f\"True values: {y_true_regression}\")\n",
    "print(f\"Predicted values: {y_pred_regression}\")\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = tf.reduce_mean(tf.square(y_true_regression - y_pred_regression))\n",
    "print(f\"MSE: {mse}\")\n",
    "\n",
    "# Mean Absolute Error (MAE)  \n",
    "mae = tf.reduce_mean(tf.abs(y_true_regression - y_pred_regression))\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# Huber Loss (robust to outliers)\n",
    "huber_loss_fn = tf.keras.losses.Huber()\n",
    "huber = huber_loss_fn(y_true_regression, y_pred_regression)\n",
    "print(f\"Huber Loss: {huber}\")\n",
    "\n",
    "# Classification loss functions\n",
    "y_true_class = tf.constant([0, 1, 2, 1])  # Class indices\n",
    "y_pred_logits = tf.constant([[2.0, 0.5, 0.1],   # Logits for class 0\n",
    "                             [0.1, 2.0, 0.3],   # Logits for class 1\n",
    "                             [0.2, 0.1, 1.8],   # Logits for class 2\n",
    "                             [0.3, 1.5, 0.2]])  # Logits for class 1\n",
    "\n",
    "print(f\"\\nClassification Loss Functions:\")\n",
    "print(f\"True classes: {y_true_class}\")\n",
    "print(f\"Predicted logits:\\n{y_pred_logits}\")\n",
    "\n",
    "# Sparse categorical crossentropy (for integer labels)\n",
    "sparse_ce_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "sparse_ce = sparse_ce_loss_fn(y_true_class, y_pred_logits)\n",
    "print(f\"Sparse Categorical Crossentropy: {sparse_ce}\")\n",
    "\n",
    "# Convert to one-hot for categorical crossentropy\n",
    "y_true_onehot = tf.one_hot(y_true_class, depth=3)\n",
    "categorical_ce_loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "categorical_ce = categorical_ce_loss_fn(y_true_onehot, y_pred_logits)\n",
    "print(f\"Categorical Crossentropy: {categorical_ce}\")\n",
    "\n",
    "# Binary classification\n",
    "y_true_binary = tf.constant([0.0, 1.0, 1.0, 0.0])\n",
    "y_pred_binary = tf.constant([0.1, 0.9, 0.8, 0.2])\n",
    "\n",
    "binary_ce_loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "binary_ce = binary_ce_loss_fn(y_true_binary, y_pred_binary)\n",
    "print(f\"\\nBinary Crossentropy: {binary_ce}\")\n",
    "\n",
    "# Custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"Custom loss: weighted MSE\"\"\"\n",
    "    weights = tf.constant([1.0, 2.0, 1.5, 0.5])\n",
    "    squared_diff = tf.square(y_true - y_pred)\n",
    "    weighted_squared_diff = weights * squared_diff\n",
    "    return tf.reduce_mean(weighted_squared_diff)\n",
    "\n",
    "custom = custom_loss(y_true_regression, y_pred_regression)\n",
    "print(f\"\\nCustom weighted MSE: {custom}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a7f58",
   "metadata": {},
   "source": [
    "## 11. Gradient Computation and Automatic Differentiation\n",
    "\n",
    "TensorFlow's automatic differentiation (autodiff) is fundamental for training neural networks. It allows us to compute gradients automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb7afd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 3.0\n",
      "y = x^2 + 2x + 1 = 16.0\n",
      "dy/dx = 2x + 2 = 8.0\n",
      "\n",
      "Multiple variables:\n",
      "x1 = 2.0, x2 = 3.0\n",
      "z = x1^2 + x2^3 + 2*x1*x2 = 43.0\n",
      "dz/dx1 = 10.0\n",
      "dz/dx2 = 31.0\n",
      "\n",
      "Vector function:\n",
      "x = [1. 2. 3.]\n",
      "y = sum(x^2) = 14.0\n",
      "dy/dx = [2. 4. 6.]\n",
      "\n",
      "Higher-order gradients:\n",
      "x = 2.0\n",
      "y = x^4 = 16.0\n",
      "dy/dx = 4x^3 = 32.0\n",
      "dy/dx = 12x^2 = 48.0\n",
      "\n",
      "Linear regression gradients:\n",
      "Initial w = [-0.98013866], b = [0.]\n",
      "Loss = 66.60920715332031\n",
      "dL/dw = [-44.70208]\n",
      "dL/db = [-14.900694]\n"
     ]
    }
   ],
   "source": [
    "# Simple gradient computation\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# Record operations for gradient computation\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2 + 2 * x + 1\n",
    "\n",
    "# Compute gradient dy/dx\n",
    "gradient = tape.gradient(y, x)\n",
    "print(f\"x = {x.numpy()}\")\n",
    "print(f\"y = x^2 + 2x + 1 = {y.numpy()}\")\n",
    "print(f\"dy/dx = 2x + 2 = {gradient.numpy()}\")\n",
    "\n",
    "# Multiple variables\n",
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = x1**2 + x2**3 + 2*x1*x2\n",
    "\n",
    "# Compute gradients with respect to both variables\n",
    "gradients = tape.gradient(z, [x1, x2])\n",
    "print(f\"\\nMultiple variables:\")\n",
    "print(f\"x1 = {x1.numpy()}, x2 = {x2.numpy()}\")\n",
    "print(f\"z = x1^2 + x2^3 + 2*x1*x2 = {z.numpy()}\")\n",
    "print(f\"dz/dx1 = {gradients[0].numpy()}\")\n",
    "print(f\"dz/dx2 = {gradients[1].numpy()}\")\n",
    "\n",
    "# Gradient of a vector function\n",
    "x_vec = tf.Variable([1.0, 2.0, 3.0])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y_vec = tf.reduce_sum(x_vec**2)\n",
    "\n",
    "grad_vec = tape.gradient(y_vec, x_vec)\n",
    "print(f\"\\nVector function:\")\n",
    "print(f\"x = {x_vec.numpy()}\")\n",
    "print(f\"y = sum(x^2) = {y_vec.numpy()}\")\n",
    "print(f\"dy/dx = {grad_vec.numpy()}\")\n",
    "\n",
    "# Higher-order gradients\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape() as tape1:\n",
    "        y = x**4\n",
    "    first_grad = tape1.gradient(y, x)  # First derivative\n",
    "second_grad = tape2.gradient(first_grad, x)  # Second derivative\n",
    "\n",
    "print(f\"\\nHigher-order gradients:\")\n",
    "print(f\"x = {x.numpy()}\")\n",
    "print(f\"y = x^4 = {y.numpy()}\")\n",
    "print(f\"dy/dx = 4x^3 = {first_grad.numpy()}\")\n",
    "print(f\"dy/dx = 12x^2 = {second_grad.numpy()}\")\n",
    "\n",
    "# Gradient computation with loss function (mini training example)\n",
    "# Simple linear regression: y = wx + b\n",
    "w = tf.Variable(tf.random.normal([1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# Sample data\n",
    "x_data = tf.constant([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_true = tf.constant([[2.0], [4.0], [6.0], [8.0]])  # y = 2x\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y_pred = x_data * w + b\n",
    "    loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Compute gradients\n",
    "gradients = tape.gradient(loss, [w, b])\n",
    "print(f\"\\nLinear regression gradients:\")\n",
    "print(f\"Initial w = {w.numpy()}, b = {b.numpy()}\")\n",
    "print(f\"Loss = {loss.numpy()}\")\n",
    "print(f\"dL/dw = {gradients[0].numpy()}\")\n",
    "print(f\"dL/db = {gradients[1].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d1b0a",
   "metadata": {},
   "source": [
    "## 12. Random Number Generation\n",
    "\n",
    "TensorFlow provides various functions for generating random numbers, which are essential for initialization and data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f75a6c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal random (mean=0, stddev=1):\n",
      "[[ 0.3274685 -0.8426258  0.3194337]\n",
      " [-1.4075519 -2.3880599 -1.0392479]\n",
      " [-0.5573232  0.539707   1.6994323]]\n",
      "\n",
      "Uniform random [0, 1):\n",
      "[[0.68789124 0.48447883 0.9309944  0.252187  ]\n",
      " [0.73115396 0.89256823 0.94674826 0.7493341 ]]\n",
      "\n",
      "Random integers [1, 10):\n",
      "[[9 9 8]\n",
      " [5 8 4]]\n",
      "\n",
      "Truncated normal:\n",
      "[[ 0.65648675 -0.4130517   0.33997506]\n",
      " [-1.0056272   1.6890494   1.1180437 ]]\n",
      "\n",
      "Original sequence: [0 1 2 3 4 5 6 7 8 9]\n",
      "Shuffled sequence: [9 4 8 1 0 2 7 3 5 6]\n",
      "\n",
      "Dropout simulation:\n",
      "Input tensor:\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "Random mask:\n",
      "[[ True False  True False]\n",
      " [ True  True False  True]\n",
      " [False  True  True False]]\n",
      "After dropout:\n",
      "[[1. 0. 1. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [0. 1. 1. 0.]]\n",
      "\n",
      "Categorical sampling:\n",
      "Logits:\n",
      "[[1.  2.  3. ]\n",
      " [1.5 0.5 2. ]]\n",
      "Samples (5 per row):\n",
      "[[2 2 2 2 2]\n",
      " [2 2 1 2 0]]\n",
      "\n",
      "Random normal with different means and stds:\n",
      "Target means: [1. 2. 3.]\n",
      "Target stds: [0.1 0.5 1. ]\n",
      "Actual means: [0.9859023 1.9779978 3.025094 ]\n",
      "Actual stds: [0.09437504 0.5608138  0.9939234 ]\n",
      "\n",
      "Bernoulli samples (p=0.7):\n",
      "[[ True False False]\n",
      " [ True False False]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Normal (Gaussian) distribution\n",
    "normal_random = tf.random.normal((3, 3), mean=0.0, stddev=1.0)\n",
    "print(f\"Normal random (mean=0, stddev=1):\\n{normal_random}\")\n",
    "\n",
    "# Uniform distribution\n",
    "uniform_random = tf.random.uniform((2, 4), minval=0.0, maxval=1.0)\n",
    "print(f\"\\nUniform random [0, 1):\\n{uniform_random}\")\n",
    "\n",
    "# Random integers\n",
    "random_ints = tf.random.uniform((2, 3), minval=1, maxval=10, dtype=tf.int32)\n",
    "print(f\"\\nRandom integers [1, 10):\\n{random_ints}\")\n",
    "\n",
    "# Truncated normal (values more than 2 std devs are re-drawn)\n",
    "truncated_normal = tf.random.truncated_normal((2, 3), mean=0.0, stddev=1.0)\n",
    "print(f\"\\nTruncated normal:\\n{truncated_normal}\")\n",
    "\n",
    "# Random shuffle\n",
    "sequence = tf.range(10)\n",
    "shuffled = tf.random.shuffle(sequence)\n",
    "print(f\"\\nOriginal sequence: {sequence}\")\n",
    "print(f\"Shuffled sequence: {shuffled}\")\n",
    "\n",
    "# Dropout simulation (randomly set elements to zero)\n",
    "input_tensor = tf.ones((3, 4))\n",
    "dropout_rate = 0.3\n",
    "random_mask = tf.random.uniform((3, 4)) > dropout_rate\n",
    "dropout_output = input_tensor * tf.cast(random_mask, tf.float32)\n",
    "\n",
    "print(f\"\\nDropout simulation:\")\n",
    "print(f\"Input tensor:\\n{input_tensor}\")\n",
    "print(f\"Random mask:\\n{random_mask}\")\n",
    "print(f\"After dropout:\\n{dropout_output}\")\n",
    "\n",
    "# Categorical sampling\n",
    "logits = tf.constant([[1.0, 2.0, 3.0], [1.5, 0.5, 2.0]])\n",
    "samples = tf.random.categorical(logits, num_samples=5)\n",
    "print(f\"\\nCategorical sampling:\")\n",
    "print(f\"Logits:\\n{logits}\")\n",
    "print(f\"Samples (5 per row):\\n{samples}\")\n",
    "\n",
    "# Random normal with different parameters\n",
    "mean_tensor = tf.constant([1.0, 2.0, 3.0])\n",
    "std_tensor = tf.constant([0.1, 0.5, 1.0])\n",
    "random_with_params = tf.random.normal((100, 3)) * std_tensor + mean_tensor\n",
    "\n",
    "print(f\"\\nRandom normal with different means and stds:\")\n",
    "print(f\"Target means: {mean_tensor}\")\n",
    "print(f\"Target stds: {std_tensor}\")\n",
    "print(f\"Actual means: {tf.reduce_mean(random_with_params, axis=0)}\")\n",
    "print(f\"Actual stds: {tf.math.reduce_std(random_with_params, axis=0)}\")\n",
    "\n",
    "# Bernoulli distribution (useful for binary masks)\n",
    "prob = 0.7\n",
    "bernoulli_samples = tf.random.uniform((3, 3)) < prob\n",
    "print(f\"\\nBernoulli samples (p={prob}):\\n{bernoulli_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e9f0b",
   "metadata": {},
   "source": [
    "## 13. Conditional Operations\n",
    "\n",
    "TensorFlow provides conditional operations that allow you to build dynamic computational graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbc5fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: [ True False  True False]\n",
      "x: [1 2 3 4]\n",
      "y: [10 20 30 40]\n",
      "tf.where result: [ 1 20  3 40]\n",
      "\n",
      "Values: [-2. -1.  0.  1.  2.]\n",
      "Positive mask: [False False False  True  True]\n",
      "Filtered (negatives become 0): [0. 0. 0. 1. 2.]\n",
      "\n",
      "x = 5.0\n",
      "tf.cond result (x > 3): 25.0\n",
      "case(-1.0): negative\n",
      "case(0.0): zero\n",
      "case(1.0): positive\n",
      "\n",
      "Original values: [-5. -1.  0.  1.  5.]\n",
      "Clipped to [-2, 2]: [-2. -1.  0.  1.  2.]\n",
      "\n",
      "Boolean operations:\n",
      "a: [ True  True False False]\n",
      "b: [ True False  True False]\n",
      "a AND b: [ True False False False]\n",
      "a OR b: [ True  True  True False]\n",
      "NOT a: [False False  True  True]\n",
      "\n",
      "Data:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Mask: [ True False  True]\n",
      "Masked data:\n",
      "[[1 2 3]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "# tf.where: element-wise conditional selection\n",
    "condition = tf.constant([True, False, True, False])\n",
    "x = tf.constant([1, 2, 3, 4])\n",
    "y = tf.constant([10, 20, 30, 40])\n",
    "\n",
    "result = tf.where(condition, x, y)\n",
    "print(f\"Condition: {condition}\")\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"tf.where result: {result}\")\n",
    "\n",
    "# Numerical condition\n",
    "values = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "positive_mask = values > 0\n",
    "\n",
    "# Select positive values, replace negative with zero\n",
    "filtered = tf.where(positive_mask, values, 0.0)\n",
    "print(f\"\\nValues: {values}\")\n",
    "print(f\"Positive mask: {positive_mask}\")\n",
    "print(f\"Filtered (negatives become 0): {filtered}\")\n",
    "\n",
    "# tf.cond: conditional execution (like if-else)\n",
    "x = tf.constant(5.0)\n",
    "\n",
    "def true_fn():\n",
    "    return tf.square(x)\n",
    "\n",
    "def false_fn():\n",
    "    return tf.sqrt(x)\n",
    "\n",
    "# This will execute true_fn since x > 3\n",
    "result_cond = tf.cond(x > 3, true_fn, false_fn)\n",
    "print(f\"\\nx = {x}\")\n",
    "print(f\"tf.cond result (x > 3): {result_cond}\")\n",
    "\n",
    "# tf.case: multiple conditions (like switch statement)\n",
    "def case_example(x):\n",
    "    return tf.case([\n",
    "        (tf.less(x, 0), lambda: tf.constant(\"negative\")),\n",
    "        (tf.equal(x, 0), lambda: tf.constant(\"zero\")),\n",
    "        (tf.greater(x, 0), lambda: tf.constant(\"positive\"))\n",
    "    ], default=lambda: tf.constant(\"unknown\"))\n",
    "\n",
    "test_values = [-1.0, 0.0, 1.0]\n",
    "for val in test_values:\n",
    "    result = case_example(tf.constant(val))\n",
    "    print(f\"case({val}): {result.numpy().decode()}\")\n",
    "\n",
    "# Clipping values (commonly used operation)\n",
    "values_to_clip = tf.constant([-5.0, -1.0, 0.0, 1.0, 5.0])\n",
    "clipped = tf.clip_by_value(values_to_clip, -2.0, 2.0)\n",
    "print(f\"\\nOriginal values: {values_to_clip}\")\n",
    "print(f\"Clipped to [-2, 2]: {clipped}\")\n",
    "\n",
    "# Boolean operations\n",
    "a = tf.constant([True, True, False, False])\n",
    "b = tf.constant([True, False, True, False])\n",
    "\n",
    "logical_and = tf.logical_and(a, b)\n",
    "logical_or = tf.logical_or(a, b)\n",
    "logical_not = tf.logical_not(a)\n",
    "\n",
    "print(f\"\\nBoolean operations:\")\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"a AND b: {logical_and}\")\n",
    "print(f\"a OR b: {logical_or}\")\n",
    "print(f\"NOT a: {logical_not}\")\n",
    "\n",
    "# Masking and advanced indexing\n",
    "data = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "mask = tf.constant([True, False, True])\n",
    "\n",
    "# Select rows based on mask\n",
    "masked_data = tf.boolean_mask(data, mask)\n",
    "print(f\"\\nData:\\n{data}\")\n",
    "print(f\"Mask: {mask}\")\n",
    "print(f\"Masked data:\\n{masked_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ade322",
   "metadata": {},
   "source": [
    "## 14. Summary and Best Practices\n",
    "\n",
    "This notebook covered the fundamental mathematical operations and concepts in TensorFlow:\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Tensor Creation**: Scalars, vectors, matrices, and higher-dimensional tensors\n",
    "2. **Variables vs Constants**: Understanding mutable vs immutable tensors\n",
    "3. **Mathematical Operations**: Arithmetic, reduction, trigonometric, and logarithmic functions\n",
    "4. **Linear Algebra**: Matrix operations, decompositions, and transformations\n",
    "5. **Tensor Manipulation**: Reshaping, broadcasting, concatenation, and splitting\n",
    "6. **Activation Functions**: Common neural network activation functions\n",
    "7. **Loss Functions**: Regression and classification loss functions\n",
    "8. **Automatic Differentiation**: Gradient computation for optimization\n",
    "9. **Random Operations**: Number generation and sampling techniques\n",
    "10. **Conditional Operations**: Dynamic graph construction with conditions\n",
    "\n",
    "### Best Practices:\n",
    "- Always specify data types explicitly when precision matters\n",
    "- Use broadcasting instead of explicit expansion when possible\n",
    "- Prefer tf.Variable for trainable parameters and tf.constant for fixed values\n",
    "- Use tf.GradientTape for custom training loops and gradient computation\n",
    "- Set random seeds for reproducible experiments\n",
    "- Use appropriate activation and loss functions for your specific task\n",
    "- Leverage TensorFlow's automatic differentiation for gradient-based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74bfa374",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '' (U+1F525) (3669007531.py, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[31], line 48\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"You now understand the fundamental mathematical operations in TensorFlow!\")\u001b[0m\n\u001b[1;37m                                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '' (U+1F525)\n"
     ]
    }
   ],
   "source": [
    "# Final example: Putting it all together\n",
    "# Simple neural network forward pass with all the concepts we've learned\n",
    "\n",
    "# Initialize parameters\n",
    "input_size = 4\n",
    "hidden_size = 3\n",
    "output_size = 2\n",
    "batch_size = 2\n",
    "\n",
    "# Random input data (batch_size, input_size)\n",
    "X = tf.random.normal((batch_size, input_size))\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal((input_size, hidden_size)) * 0.1, name=\"W1\")\n",
    "b1 = tf.Variable(tf.zeros((hidden_size,)), name=\"b1\")\n",
    "W2 = tf.Variable(tf.random.normal((hidden_size, output_size)) * 0.1, name=\"W2\")\n",
    "b2 = tf.Variable(tf.zeros((output_size,)), name=\"b2\")\n",
    "\n",
    "print(\"Input shape:\", X.shape)\n",
    "print(\"Input data:\\n\", X)\n",
    "\n",
    "# Forward pass with automatic differentiation tracking\n",
    "with tf.GradientTape() as tape:\n",
    "    # Hidden layer\n",
    "    z1 = tf.matmul(X, W1) + b1  # Linear transformation\n",
    "    a1 = tf.nn.relu(z1)         # Activation function\n",
    "    \n",
    "    # Output layer\n",
    "    z2 = tf.matmul(a1, W2) + b2  # Linear transformation\n",
    "    output = tf.nn.softmax(z2)   # Softmax for probability distribution\n",
    "    \n",
    "    # Dummy loss for demonstration\n",
    "    target = tf.constant([[1, 0], [0, 1]], dtype=tf.float32)\n",
    "    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(target, output))\n",
    "\n",
    "print(f\"\\nHidden layer output shape: {a1.shape}\")\n",
    "print(f\"Final output shape: {output.shape}\")\n",
    "print(f\"Output probabilities:\\n{output}\")\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# Compute gradients\n",
    "gradients = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "print(f\"\\nGradient shapes:\")\n",
    "for i, grad in enumerate(gradients):\n",
    "    print(f\"Gradient {i} shape: {grad.shape}\")\n",
    "\n",
    "print(\"\\n Complete TensorFlow basics tutorial finished!\")\n",
    "print(\"You now understand the fundamental mathematical operations in TensorFlow!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
